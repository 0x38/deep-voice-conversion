시도해 볼만한 것
* magnitude normalization 시도
* 데시벨 변환
* mel-scale로 시도
* train2 loss를 spectrogram이 아닌 mcep 기준으로 하면 다른가?
* train1 Trim 부정확하게 한 부분이 문제?
* silance training
 - train1, train2에서 trim 제거하고 random_crop 추가
* softmax 빼고
* phone 경계부분을 soft하게 (timit은 hard)
* adversarial training
 - wgan
 - average되지 않은 더 sharp한 소리를 만들어 낼 것이라 기대

---
알게된 것
* spectrogram -> wav 변환시 magnitude에 어느 정도 emphasis를 주는 것은 잡음을 제거하는데 도움이 된다.
* normalization before/after RELU 혹은 normalization 종류는 크게 영향을 미치지 않는다.
* 각 모듈(net1, net2)에서 쓰는 sample rate, window_len, hop_len는 같아야 한다.
* CBHG는 tacotron 논문에도 언급되있듯이 overfitting 문제를 덜 겪는것 같다.
* softmax temperature로 ppg dist.를 임의로 바꾸는 것은 크게 도움되지 않는다.
 - 어차피 class의 수는 동일하고 0~1 사이의 범위라고는 하지만 실수 범위이기 때문에 담을 수 있는 정보량은 동일.
 - cf) autoencoder vs variational autoencoder
* train1의 classification 정확도는 완벽할 필요는 없음.
 - training set의 label이 완벽하지 않아도 (수렴하는데까지는 더 오래 걸리지만) 결국 완벽할 때의 optimal에 도달한다는 hinton의 연구 결과 참고
* phoneme classification 할땐 window_len, hop_len가 하나의 class로 매핑될 수 있게 충분히 작아야 한다. 각각이 25ms, 5ms인 이유.

아이디어
* train2에서 loss를 conversion 전후의 l2 + conversion 전후의 ppg

트러블슈팅
* queue insufficient element 문제
    * 어떤 오류로 큐에 데이터가 안 들어오고 있는 상태
    * queue를 disable 시키고 placeholder feed_dict 같은걸로 실행시켜봐야 정확한 원인을 알 수 있음
* audioread nobackend error
    * 다른 코덱이 필요할 때. wav가 16bit가 아니라 32bit이면 다른 코덱을 쓰는 듯

엔지니어링 이슈
* case별 hp 세팅파일
** dataset 선택 파라미터로 설정 가능하게 hp에 적기
** train1/train2 model hp 나누기
** 한꺼번에 학습 시작 스크립트
* writer 합치기

* 학습 재실행시 다시 쓸때도 epoch 이어서 쓰기
* utils 패키지화
    * https://wikidocs.net/1418
